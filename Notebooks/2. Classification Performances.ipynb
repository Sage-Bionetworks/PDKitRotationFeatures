{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Performance of Aggregated Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome, Aryton Tediarjo!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import synapseclient as sc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\", 'grid.color': '.8'})\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "syn = sc.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATED_FEATURES = \"syn22331590\"\n",
    "MATCHED_HC = \"syn22254800\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_hc = pd.read_csv(\n",
    "    syn.get(MATCHED_HC)[\"path\"], sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(syn.get(AGGREGATED_FEATURES)[\"path\"], sep = \"\\t\").dropna().set_index(\"healthCode\")\n",
    "feat_used = [feat for feat in data.columns \n",
    "             if (\"createdOn\" not in feat) \n",
    "             and (\"window\" not in feat) \n",
    "             and (\"error\" not in feat) \n",
    "             and ('nrecords' not in feat)\n",
    "             and (\"healthCode\" not in feat) \n",
    "             and (\"gender\" not in feat) \n",
    "             and (\"PD\" not in feat) \n",
    "             and (\"age\" not in feat)]\n",
    "\n",
    "data = data.join(matched_hc[[\"healthCode\"]].set_index(\"healthCode\"), how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(337, 106)\n",
      "(113, 106)\n",
      "(337,)\n",
      "(113,)\n"
     ]
    }
   ],
   "source": [
    "#Seperate train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[feat_used],\n",
    "                                                   data['PD'],\n",
    "                                                   test_size = 0.25,\n",
    "                                                   random_state = 100)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "clfs.append(LogisticRegression(random_state = 100))\n",
    "clfs.append(RidgeClassifier(random_state = 100))\n",
    "clfs.append(SVC())\n",
    "clfs.append(KNeighborsClassifier(n_neighbors=3))\n",
    "clfs.append(DecisionTreeClassifier(random_state = 100))\n",
    "clfs.append(RandomForestClassifier(max_depth = 5, \n",
    "                                   random_state = 100, \n",
    "                                   n_estimators = 5000))\n",
    "clfs.append(GradientBoostingClassifier(max_depth = 5, \n",
    "                                       random_state = 100, \n",
    "                                       n_estimators = 5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Performance (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics_placeholder = {}\n",
    "model_metrics_placeholder[\"classifier\"] = []\n",
    "model_metrics_placeholder[\"cross_validation_auc_score\"] = []\n",
    "model_metrics_placeholder[\"cross_validation_std\"] = []\n",
    "model_metrics_placeholder[\"test_auc_score\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=100, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.028294706344604494\n",
      "fit_time  std  0.004595044520079132\n",
      "score_time  mean  0.002785038948059082\n",
      "score_time  std  0.00019465483506384123\n",
      "test_score  mean  0.708671679197995\n",
      "test_score  std  0.08598781269903454\n",
      "---------------------------------\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "                max_iter=None, normalize=False, random_state=100, solver='auto',\n",
      "                tol=0.001)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.006667733192443848\n",
      "fit_time  std  0.0006186417668702055\n",
      "score_time  mean  0.002469444274902344\n",
      "score_time  std  0.0001955646674237313\n",
      "test_score  mean  0.7442481203007519\n",
      "test_score  std  0.0723161383904682\n",
      "---------------------------------\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.01962766647338867\n",
      "fit_time  std  0.0023694193158764138\n",
      "score_time  mean  0.0041979789733886715\n",
      "score_time  std  0.0009887554520819526\n",
      "test_score  mean  0.6833834586466165\n",
      "test_score  std  0.05592953643444871\n",
      "---------------------------------\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "                     weights='uniform')\n",
      "-----------------------------------\n",
      "fit_time  mean  0.004731369018554687\n",
      "fit_time  std  0.0007844589942371633\n",
      "score_time  mean  0.005134725570678711\n",
      "score_time  std  0.0006707441390252115\n",
      "test_score  mean  0.5905451127819549\n",
      "test_score  std  0.13902591396476768\n",
      "---------------------------------\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=100, splitter='best')\n",
      "-----------------------------------\n",
      "fit_time  mean  0.021561908721923827\n",
      "fit_time  std  0.004008103770817834\n",
      "score_time  mean  0.002479839324951172\n",
      "score_time  std  0.0007808332791685433\n",
      "test_score  mean  0.6052443609022558\n",
      "test_score  std  0.09628437389754689\n"
     ]
    }
   ],
   "source": [
    "for classifier in clfs:\n",
    "    pipeline = Pipeline([\n",
    "        ('normalizer', MinMaxScaler()), \n",
    "        ('clf', classifier)])\n",
    "    \n",
    "    scores = cross_validate(pipeline, \n",
    "                            X_train, \n",
    "                            y_train,\n",
    "                            cv = 10, \n",
    "                           scoring = \"roc_auc\",\n",
    "                           n_jobs = -1)\n",
    "    print('---------------------------------')\n",
    "    print(str(classifier))\n",
    "    print('-----------------------------------')\n",
    "    for key, values in scores.items():\n",
    "            print(key,' mean ', values.mean())\n",
    "            print(key,' std ', values.std())\n",
    "    \n",
    "    model_metrics_placeholder[\"classifier\"].append(type(classifier).__name__)\n",
    "    model_metrics_placeholder[\"cross_validation_auc_score\"].append(scores[\"test_score\"].mean())\n",
    "    model_metrics_placeholder[\"cross_validation_std\"].append(scores[\"test_score\"].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-set Performance (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in clfs:\n",
    "    print('---------------------------------')\n",
    "    print(str(classifier))\n",
    "    print('-----------------------------------')\n",
    "    pipeline = Pipeline([\n",
    "        ('normalizer', MinMaxScaler()), \n",
    "        ('clf', classifier)])\n",
    "    auc_score = roc_auc_score(pipeline.fit(X_train, y_train).predict(X_test), y_test)\n",
    "    print(\"ROC-AUC Score on Test-Set %s \\n \\n\"%auc_score)\n",
    "    model_metrics_placeholder[\"test_auc_score\"].append(auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('normalizer', MinMaxScaler()), \n",
    "        ('clf', clfs[5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = pipeline.fit(X_train, y_train)\n",
    "proba = model_fit.predict_proba(X_test)\n",
    "proba = pd.DataFrame(proba).rename({0:\"Healthy Probability Score\", \n",
    "                                    1:\"PD Probability Score\"}, axis = 1)\n",
    "label = pd.DataFrame(y_test).reset_index(drop = True)\n",
    "proba_df = pd.concat([proba, label], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.) Assigned PD Probability between PD users and Healthy Controls (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "sns.distplot(proba_df[proba_df[\"PD\"] == 0][\"PD Probability Score\"], label = \"Healthy Controls\")\n",
    "sns.distplot(proba_df[proba_df[\"PD\"] == 1][\"PD Probability Score\"], label = \"PD\")\n",
    "sns.despine()\n",
    "\n",
    "plt.title(\"Random Forest Prediction Probability Given Healthy vs PD Users on Test-Set\")\n",
    "plt.xlabel(\"Assigned PD Probability from Model (Random Forest)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b). ROC Curves (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, proba_df[\"PD Probability Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='-.', label='Random Guess', color = \"red\")\n",
    "plt.plot(lr_fpr, lr_tpr, marker = \".\",label='Random Forest Model')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(\"ROC Curve of Random Forest Model\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
